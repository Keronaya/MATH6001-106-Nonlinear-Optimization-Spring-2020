<b>MATH6001-106. Nonlinear Optimization in Machine Learning.</b>

3. Gradient Descent v.s. Accelarated Gradient Descent on quadratic and perturbed quadratic functions.

Consider a two-dimensional quadratic function $f(x_1, x_2)=A x_1^2 + B x_2^2$ for $A, B>0$ and a small perturbation of this function $g_\epsilon(x_1, x_2)=A x_1^2 + B x_2^2 + \epsilon (x_1^2+x_2^2)^{3}$. 

